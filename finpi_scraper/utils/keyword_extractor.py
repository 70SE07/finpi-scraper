#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å—Ç–æ–ø-—Å–ª–æ–≤.
"""

import json
import os
from collections import Counter
from datetime import datetime
import nltk
from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder
from .lemmatizer import lemmatize_text

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ---
_stopwords = {}
LANG_MAP = {"uk": "ukrainian", "ru": "russian", "en": "english"}
STOPWORD_SUGGESTION_THRESHOLD = 0.1 # –°—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –µ—Å–ª–∏ –æ–Ω–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –±–æ–ª–µ–µ —á–µ–º –≤ 10% —Ç–æ–≤–∞—Ä–æ–≤

def _load_stopwords():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞."""
    global _stopwords
    stopwords_path = os.path.join(os.path.dirname(__file__), '..', 'keywords', 'stopwords.json')
    custom_stopwords = {}
    if os.path.exists(stopwords_path):
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            custom_stopwords = json.load(f)

    for lang_code, lang_name in LANG_MAP.items():
        try:
            nltk_stopwords = set(nltk.corpus.stopwords.words(lang_name))
            custom = set(custom_stopwords.get(lang_code, []))
            _stopwords[lang_code] = nltk_stopwords.union(custom)
        except (OSError, KeyError):
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —è–∑—ã–∫–∞ '{lang_name}'.")
            _stopwords[lang_code] = set(custom_stopwords.get(lang_code, []))

_load_stopwords()

def extract_keywords_from_products(products, lang, min_freq=2):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Ñ—Ä–∞–∑—ã –∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (potential_keywords, stopword_candidates)
    """
    print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ (—è–∑—ã–∫: {lang})...")
    
    all_lemmas = []
    for product in products:
        lemmas = lemmatize_text(product, lang)
        filtered = [lemma for lemma in lemmas if lemma not in _stopwords.get(lang, set()) and len(lemma) > 2]
        all_lemmas.extend(filtered)

    unigram_freq = Counter(all_lemmas)
    
    finder = BigramCollocationFinder.from_words(all_lemmas)
    finder.apply_freq_filter(min_freq)
    bigram_measures = BigramAssocMeasures()
    found_bigrams = finder.nbest(bigram_measures.pmi, 50)

    potential_keywords = {}
    stopword_candidates = []

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∏–≥—Ä–∞–º–º - —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
    for bg in found_bigrams:
        phrase = " ".join(bg)
        potential_keywords[phrase] = finder.ngram_fd[bg]

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —É–Ω–∏–≥—Ä–∞–º–º - –º–æ–≥—É—Ç –±—ã—Ç—å –∏ –∫–ª—é—á–∞–º–∏, –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏
    stopword_freq_threshold = len(products) * STOPWORD_SUGGESTION_THRESHOLD
    for word, count in unigram_freq.items():
        if count >= min_freq:
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –°–õ–ò–®–ö–û–ú —á–∞—Å—Ç–æ, —ç—Ç–æ –∫–∞–Ω–¥–∏–¥–∞—Ç –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            if count > stopword_freq_threshold and len(products) > 10: # –ü–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫
                stopword_candidates.append(word)
            else:
                potential_keywords[word] = count

    sorted_keywords = dict(sorted(potential_keywords.items(), key=lambda item: item[1], reverse=True))
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(sorted_keywords)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π –∏ {len(stopword_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞.")
    print(f"üîù –¢–æ–ø-10 –∫–ª—é—á–µ–π: {list(sorted_keywords.items())[:10]}")
    if stopword_candidates:
        print(f"üí° –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞: {stopword_candidates}")
    
    return sorted_keywords, stopword_candidates

def analyze_other_products(other_file_path, lang='uk'):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–≤–∞—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞ OTHER. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∫–ª—é—á–∏, –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞).
    """
    if not os.path.exists(other_file_path):
        print(f"‚ùå –§–∞–π–ª {other_file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None, None
    
    with open(other_file_path, 'r', encoding='utf-8') as f:
        products = [line.strip() for line in f.readlines() if line.strip()]
    
    if not products:
        print("üìÅ –§–∞–π–ª OTHER –ø—É—Å—Ç")
        return None, None
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ OTHER")
    
    return extract_keywords_from_products(products, lang)

def update_suggested_stopwords(stopwords_file, suggestions):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –≤ `suggested_stopwords.json`.
    """
    if not suggestions:
        return

    if os.path.exists(stopwords_file):
        with open(stopwords_file, 'r', encoding='utf-8') as f:
            try:
                existing = set(json.load(f))
            except json.JSONDecodeError:
                existing = set()
    else:
        existing = set()
        
    new_suggestions = [s for s in suggestions if s not in existing]

    if new_suggestions:
        updated_list = sorted(list(existing.union(new_suggestions)))
        with open(stopwords_file, 'w', encoding='utf-8') as f:
            json.dump(updated_list, f, ensure_ascii=False, indent=2)
        print(f"‚úçÔ∏è –í `suggested_stopwords.json` –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(new_suggestions)} –Ω–æ–≤—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {new_suggestions}")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ OTHER.
    """
    print("üîç –ê–ù–ê–õ–ò–ó –¢–û–í–ê–†–û–í –ò–ó –ö–ê–¢–ï–ì–û–†–ò–ò OTHER")
    print("="*50)
    
    other_file = "output/GOODS/GROCERIES/BEVERAGES/rozetka_alcohol_other.txt"
    keywords_file = "keywords/alcohol_keywords.json"
    suggested_stopwords_file = "keywords/suggested_stopwords.json"
    
    keywords, stopwords = analyze_other_products(other_file, lang='uk')
    
    if keywords:
        update_keywords_file(keywords_file, keywords, "other_analysis")
    if stopwords:
        update_suggested_stopwords(suggested_stopwords_file, stopwords)
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()
