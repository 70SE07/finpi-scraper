import requests
import json
import os
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import time

def get_scraperapi_url(site_config, page_url=None):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ü–µ–ª–µ–≤–æ–π URL –≤ URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ ScraperAPI,
    –¥–æ–±–∞–≤–ª—è—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JS –∏ –ø—Ä–µ–º–∏—É–º-–ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
    """
    load_dotenv()
    api_key = os.getenv("SCRAPERAPI_KEY")
    if not api_key or "–í–ê–®_API_–ö–õ–Æ–ß" in api_key:
        print("–û—à–∏–±–∫–∞: API-–∫–ª—é—á ScraperAPI –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ –∏–∑–º–µ–Ω–µ–Ω –≤ .env —Ñ–∞–π–ª–µ.")
        return None
    
    target_url = page_url if page_url else site_config['url']
    country = site_config.get('country_code', 'ua')
    base_url = f'http://api.scraperapi.com?api_key={api_key}&url={target_url}&country_code={country}'
    
    if site_config.get('js_rendering', False):
        print(f"[{site_config['site_name']}] –í–∫–ª—é—á–∞—é JS Rendering.")
        base_url += '&render=true'

    if site_config['site_name'] == 'tesco':
        print(f"[{site_config['site_name']}] –ò—Å–ø–æ–ª—å–∑—É—é –ø—Ä–µ–º–∏—É–º –ø—Ä–æ–∫—Å–∏.")
        base_url += '&premium=true&render_wait=5000'
    
    return base_url

def create_category_folders(category_path):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –ø–∞–ø–æ–∫ –ø–æ category_path.
    –ù–∞–ø—Ä–∏–º–µ—Ä: GOODS/GROCERIES/BEVERAGES
    """
    output_dir = "output"
    full_path = os.path.join(output_dir, category_path)
    
    if not os.path.exists(full_path):
        os.makedirs(full_path, exist_ok=True)
        print(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {full_path}")
    
    return full_path

def load_existing_products(output_filename):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤.
    """
    if not os.path.exists(output_filename):
        print(f"–§–∞–π–ª {output_filename} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ù–∞—á–∏–Ω–∞—é —Å –Ω—É–ª—è.")
        return []
    
    try:
        with open(output_filename, 'r', encoding='utf-8') as f:
            existing_products = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(existing_products)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ")
        return existing_products
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {output_filename}: {e}")
        return []

def parse_site_with_pagination(site_config):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–≤–∞—Ä–æ–≤.
    """
    site_name = site_config['site_name']
    category_name = site_config['category_name']
    target_count = site_config['target_count']
    selector = site_config['product_name_selector']
    pagination_template = site_config['pagination_template']
    category_path = site_config['category_path']
    
    print(f"--- [ScraperAPI] –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞: {site_name} ({category_name}) ---")
    print(f"–¶–µ–ª—å: —Å–æ–±—Ä–∞—Ç—å {target_count} —Ç–æ–≤–∞—Ä–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    output_path = create_category_folders(category_path)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    filename = f"{site_name}_{category_name}.txt"
    output_filename = os.path.join(output_path, filename)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    existing_products = load_existing_products(output_filename)
    all_product_names = existing_products.copy()
    
    if existing_products:
        print(f"üîÑ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –ø—Ä–æ–¥–æ–ª–∂–∞—é —Å {len(existing_products)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"üéØ –¶–µ–ª—å: –¥–æ–±—Ä–∞—Ç—å –¥–æ {target_count} —Ç–æ–≤–∞—Ä–æ–≤ (–Ω—É–∂–Ω–æ –µ—â–µ {target_count - len(existing_products)})")
    else:
        print(f"üÜï –ù–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –Ω–∞—á–∏–Ω–∞—é —Å –Ω—É–ª—è –¥–æ {target_count} —Ç–æ–≤–∞—Ä–æ–≤")
    page = 1
    max_pages = 50  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    
    while len(all_product_names) < target_count and page <= max_pages:
        print(f"[{site_name}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if page == 1:
            page_url = site_config['url']
        else:
            if pagination_template.startswith('/'):
                # –î–ª—è Rozetka: /page=2/
                page_url = site_config['url'].rstrip('/') + pagination_template.format(page=page)
            else:
                # –î–ª—è Tesco: ?page=2
                if '?' in site_config['url']:
                    page_url = site_config['url'] + '&' + pagination_template.format(page=page)
                else:
                    page_url = site_config['url'] + pagination_template.format(page=page)
        
        print(f"[{site_name}] URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_url}")
        
        api_url = get_scraperapi_url(site_config, page_url)
        if not api_url:
            break
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=120)
            response.raise_for_status()
            
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')

            product_elements = []
            # –ï—Å–ª–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä - —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –ø—Ä–æ–±—É–µ–º –∫–∞–∂–¥—ã–π –ø–æ –æ—á–µ—Ä–µ–¥–∏
            if isinstance(selector, list):
                for s in selector:
                    product_elements = soup.select(s)
                    if product_elements:
                        print(f"[{site_name}] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä: '{s}'")
                        break
            else:
                # –ò–Ω–∞—á–µ —Ä–∞–±–æ—Ç–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
                product_elements = soup.select(selector)
            
            page_products = [elem.get_text(strip=True) for elem in product_elements if elem.get_text(strip=True)]
            
            if not page_products:
                print(f"[{site_name}] –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} —Ç–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–≤–µ—Ä—à–∞—é –ø–∞—Ä—Å–∏–Ω–≥.")
                break
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã (–∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤) —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            new_products = []
            for product in page_products:
                if product not in all_product_names:
                    new_products.append(product)
                    all_product_names.append(product)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                    if len(all_product_names) >= target_count:
                        print(f"[{site_name}] ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(all_product_names)} —Ç–æ–≤–∞—Ä–æ–≤")
                        break
            
            print(f"[{site_name}] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(page_products)} —Ç–æ–≤–∞—Ä–æ–≤, –Ω–æ–≤—ã—Ö: {len(new_products)}")
            print(f"[{site_name}] –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_product_names)} –∏–∑ {target_count}")
            
            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞
            if len(all_product_names) >= target_count:
                break
            

                
        except requests.exceptions.RequestException as e:
            print(f"[{site_name}] –û—à–∏–±–∫–∞ —Å–µ—Ç–µ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
            break
        except Exception as e:
            print(f"[{site_name}] –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
            break
        
        page += 1
        time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º)
    with open(output_filename, 'w', encoding='utf-8') as f:
        for name in all_product_names:
            f.write(name + '\n')
    
    new_products_count = len(all_product_names) - len(existing_products)
    print(f"--- –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ {site_name} –∑–∞–≤–µ—Ä—à–µ–Ω. ---")
    print(f"üìä –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_product_names)} (–¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {new_products_count})")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_filename}")
    return all_product_names

import sys

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤.
    –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–º—è —Å–∞–π—Ç–∞ –∫–∞–∫ –∞—Ä–≥—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–µ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–≥–æ.
    –ü—Ä–∏–º–µ—Ä: python main.py rost
    """
    with open('config.json', 'r', encoding='utf-8') as f:
        configs = json.load(f)

    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥–∏, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –∏–º—è —Å–∞–π—Ç–∞ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö
    if len(sys.argv) > 1:
        target_site_name = sys.argv[1]
        configs = [c for c in configs if c['site_name'] == target_site_name]
        if not configs:
            print(f"‚ùå –°–∞–π—Ç '{target_site_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ config.json.")
            return
    
    all_results = {}
    
    for site_config in configs:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω –ª–∏ —Å–∞–π—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        if not site_config.get('enabled', True):
            print(f"\n{'='*60}")
            print(f"–ü–†–û–ü–£–°–ö–ê–Æ: {site_config['site_name'].upper()} - {site_config['category_name'].upper()}")
            print(f"–ü–†–ò–ß–ò–ù–ê: –û—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (enabled: false)")
            print(f"{'='*60}")
            continue
            
        site_name = site_config['site_name']
        category_name = site_config['category_name']
        target_count = site_config['target_count']
        
        print(f"\n{'='*60}")
        print(f"–ù–ê–ß–ò–ù–ê–Æ –ü–ê–†–°–ò–ù–ì: {site_name.upper()} - {category_name.upper()}")
        print(f"–¶–ï–õ–¨: {target_count} —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"{'='*60}")
        
        names = parse_site_with_pagination(site_config)
        all_results[f"{site_name}_{category_name}"] = names
        
    print(f"\n\n{'='*60}")
    print("–ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢")
    print(f"{'='*60}")
    
    total_products = 0
    for site_category, names in all_results.items():
        print(f"{site_category}: {len(names)} —Ç–æ–≤–∞—Ä–æ–≤")
        total_products += len(names)
    
    print(f"\n–û–ë–©–ò–ô –ò–¢–û–ì: {total_products} —Ç–æ–≤–∞—Ä–æ–≤")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ output/")

if __name__ == "__main__":
    main()
