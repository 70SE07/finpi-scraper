import json
import os
from dotenv import load_dotenv
import time
import sys
import logging
import asyncio
import aiohttp
import aiofiles

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–±—Ä–∏–∫—É —Å–∫—Ä–µ–π–ø–µ—Ä–æ–≤
from scrapers import get_scraper
from utils.categorization import categorize_product

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
BATCH_SIZE = 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
REQUEST_TIMEOUT = 120  # –¢–∞–π–º–∞—É—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
MAX_RETRIES = 3  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
RETRY_DELAY = 5  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

def get_scraperapi_url(site_config, page_url=None):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ü–µ–ª–µ–≤–æ–π URL –≤ URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ ScraperAPI.
    """
    load_dotenv()
    api_key = os.getenv("SCRAPERAPI_KEY")
    if not api_key or "–í–ê–®_API_–ö–õ–Æ–ß" in api_key:
        logging.error("API-–∫–ª—é—á ScraperAPI –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ –∏–∑–º–µ–Ω–µ–Ω –≤ .env —Ñ–∞–π–ª–µ.")
        return None
    
    target_url = page_url if page_url else site_config['url']
    country = site_config.get('country_code', 'ua')
    base_url = f'http://api.scraperapi.com?api_key={api_key}&url={target_url}&country_code={country}'
    
    if site_config.get('js_rendering', False):
        base_url += '&render=true'

    if site_config['site_name'] in ['tesco', 'winestyle', 'rozetka']:
        base_url += '&premium=true&render_wait=5000'
    
    return base_url

async def create_category_folders(category_path):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ–∑–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –ø–∞–ø–æ–∫.
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "output")
    full_path = os.path.join(output_dir, category_path)
    
    if not os.path.exists(full_path):
        os.makedirs(full_path, exist_ok=True)
        logging.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {category_path}")
    
    return full_path

async def load_existing_products(output_filename):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞.
    """
    if not os.path.exists(output_filename):
        logging.info(f"–§–∞–π–ª {os.path.basename(output_filename)} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ù–∞—á–∏–Ω–∞—é —Å –Ω—É–ª—è.")
        return []
    
    try:
        async with aiofiles.open(output_filename, 'r', encoding='utf-8') as f:
            lines = await f.readlines()
            existing_products = [line.strip() for line in lines if line.strip()]
        
        logging.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(existing_products)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ {os.path.basename(output_filename)}")
        return existing_products
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {os.path.basename(output_filename)}: {e}")
        return []

async def load_external_keywords(keywords_file):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–∞–∫ –µ—Å—Ç—å.
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    full_keywords_path = os.path.join(script_dir, keywords_file)

    if not os.path.exists(full_keywords_path):
        logging.warning(f"–§–∞–π–ª –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {keywords_file}")
        return None
    
    try:
        async with aiofiles.open(full_keywords_path, 'r', encoding='utf-8') as f:
            content = await f.read()
            return json.loads(content)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ {keywords_file}: {e}")
        return None



async def save_products_by_subcategory(all_products, site_name, group_name, category_path, subcategory_keywords, lang):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–≤–∞—Ä—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
    """
    output_path = await create_category_folders(category_path)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤
    subcategory_products = {}
    if subcategory_keywords:
        for product in all_products:
            subcategory = categorize_product(product, subcategory_keywords, lang)
            if subcategory not in subcategory_products:
                subcategory_products[subcategory] = []
            subcategory_products[subcategory].append(product)
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è –≥—Ä—É–ø–ø—ã –∫–∞–∫ –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        subcategory_products[group_name] = all_products

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    tasks = []
    for subcategory, products in subcategory_products.items():
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º group_name –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        filename = f"{site_name}_{group_name}_{subcategory}.txt"
        output_filename = os.path.join(output_path, filename)
        
        async def write_file(fname, prods):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'a' –¥–ª—è –¥–æ–∑–∞–ø–∏—Å–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å, –Ω–æ 'w' –ø—Ä–æ—â–µ –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ –≥—Ä—É–ø–ø—ã
            async with aiofiles.open(fname, 'w', encoding='utf-8') as f:
                await f.write('\n'.join(prods) + '\n')
            logging.info(f"üíæ {subcategory.upper()}: {len(prods)} —Ç–æ–≤–∞—Ä–æ–≤ ‚Üí {os.path.basename(fname)}")
        
        tasks.append(write_file(output_filename, products))
    
    await asyncio.gather(*tasks)
    logging.info(f"üìä –í—Å–µ–≥–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(subcategory_products)}")

    # –ê–Ω–∞–ª–∏–∑ 'other' –¥–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if subcategory_keywords and 'other' in subcategory_products and len(subcategory_products['other']) > 0:
        logging.info(f"üîç –ó–∞–ø—É—Å–∫–∞—é –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è {len(subcategory_products['other'])} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ OTHER...")
        try:
            from utils.keyword_extractor import analyze_other_products, update_suggested_stopwords
            
            other_file = os.path.join(output_path, f"{site_name}_{group_name}_other.txt")
            script_dir = os.path.dirname(os.path.abspath(__file__))
            
            new_keywords, stopword_candidates = analyze_other_products(other_file, lang)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª, –∞ –Ω–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π
            if new_keywords:
                analysis_results_path = os.path.join(script_dir, "keywords", "other_analysis_results.json")
                async with aiofiles.open(analysis_results_path, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(new_keywords, ensure_ascii=False, indent=2))
                logging.info(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {os.path.basename(analysis_results_path)}")

            if stopword_candidates:
                suggested_stopwords_path = os.path.join(script_dir, "keywords/suggested_stopwords.json")
                update_suggested_stopwords(suggested_stopwords_path, stopword_candidates)

        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ OTHER: {e}", exc_info=True)

async def fetch_page(session, url, site_name, page_num):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ª–æ–≥–∏–∫–æ–π –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫."""
    if not url:
        return None
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    for attempt in range(MAX_RETRIES):
        try:
            async with session.get(url, headers=headers, timeout=REQUEST_TIMEOUT) as response:
                response.raise_for_status()
                logging.info(f"[{site_name}] –°—Ç—Ä. {page_num}: —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Å—Ç–∞—Ç—É—Å {response.status})")
                return await response.text()
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            if attempt < MAX_RETRIES - 1:
                logging.warning(f"[{site_name}] –°—Ç—Ä. {page_num}: –æ—à–∏–±–∫–∞ '{e}', –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1} –∏–∑ {MAX_RETRIES}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫...")
                await asyncio.sleep(RETRY_DELAY)
            else:
                logging.error(f"[{site_name}] –°—Ç—Ä. {page_num}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫. –û—à–∏–±–∫–∞: {e}")
                return None
    return None

async def parse_site_with_pagination(site_config, existing_products_set):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç —Å–∞–π—Ç —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞–∫–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤.
    """
    site_name = site_config['site_name']
    category_name = site_config['category_name']
    target_count = site_config['target_count']
    
    logging.info(f"--- –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥: {site_name} ({category_name}) ---")
    logging.info(f"–¶–µ–ª—å: {target_count} —Ç–æ–≤–∞—Ä–æ–≤")

    try:
        scraper = get_scraper(site_config)
    except ValueError as e:
        logging.error(e)
        return []

    page = 1
    max_pages = 100
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π set, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –¥—É–±–ª–µ–π –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –≤ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø–µ
    local_product_names = set()

    async with aiohttp.ClientSession() as session:
        while len(local_product_names) < target_count and page <= max_pages:
            
            tasks = []
            page_numbers = range(page, page + BATCH_SIZE)
            logging.info(f"[{site_name} - {category_name}] –ì–æ—Ç–æ–≤–ª—é –ø–∞–∫–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü {page}-{page + BATCH_SIZE - 1}...")

            for p_num in page_numbers:
                page_url = scraper.get_page_url(p_num)
                api_url = get_scraperapi_url(site_config, page_url)
                tasks.append(fetch_page(session, api_url, site_name, p_num))

            results = await asyncio.gather(*tasks)
            
            new_products_found_in_batch = False
            for html_content in results:
                if html_content:
                    page_products = scraper.parse(html_content)
                    newly_added = 0
                    for product in page_products:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º, –∏ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º set
                        if product not in existing_products_set and product not in local_product_names:
                            local_product_names.add(product)
                            newly_added += 1
                            new_products_found_in_batch = True
                    
                    if newly_added > 0:
                        logging.info(f"[{site_name} - {category_name}] –ù–∞–π–¥–µ–Ω–æ {len(page_products)} —Ç–æ–≤–∞—Ä–æ–≤, –Ω–æ–≤—ã—Ö: {newly_added}")

                if len(local_product_names) >= target_count:
                    logging.info(f"[{site_name} - {category_name}] ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(local_product_names)} —Ç–æ–≤–∞—Ä–æ–≤")
                    break
            
            if not new_products_found_in_batch and page > 1:
                logging.warning(f"[{site_name} - {category_name}] –ù–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–∫–µ—Ç–µ. –ó–∞–≤–µ—Ä—à–∞—é –ø–∞—Ä—Å–∏–Ω–≥.")
                break

            if len(local_product_names) >= target_count:
                break

            page += BATCH_SIZE
            await asyncio.sleep(2) # –£–≤–µ–ª–∏—á–∏–º –ø–∞—É–∑—É –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏

    logging.info(f"--- –ü–∞—Ä—Å–∏–Ω–≥ {site_name} ({category_name}) –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–±—Ä–∞–Ω–æ: {len(local_product_names)} ---")
    return list(local_product_names)

async def process_config_group(group_key, configs):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä—É–ø–ø—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ –∞–ª–∫–æ–≥–æ–ª—å–Ω—ã–µ –Ω–∞–ø–∏—Ç–∫–∏ —Å –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞).
    """
    site_name, group_name = group_key
    logging.info(f"\n{'='*60}\nüöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≥—Ä—É–ø–ø—ã: {site_name.upper()} - {group_name.upper()}\n{'='*60}")

    all_products_in_group = set()
    
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–µ—Ä–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞ –¥–ª—è –ø—É—Ç–µ–π)
    base_config = configs[0]
    category_path = base_config['category_path']
    output_path = await create_category_folders(category_path)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–µ–π
    # –ü—Ä–∏–º–µ—Ä: rozetka_alcohol_*.txt
    import glob
    existing_files = glob.glob(os.path.join(output_path, f"{site_name}_{group_name}_*.txt"))
    for f in existing_files:
        existing_products = await load_existing_products(f)
        all_products_in_group.update(existing_products)

    initial_count = len(all_products_in_group)
    logging.info(f"[{site_name} - {group_name}] –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–æ {initial_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –≥—Ä—É–ø–ø–µ.")

    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ –≥—Ä—É–ø–ø–µ
    for config in configs:
        new_products = await parse_site_with_pagination(config, all_products_in_group)
        all_products_in_group.update(new_products)

    final_product_list = list(all_products_in_group)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ–π –≥—Ä—É–ø–ø—ã
    if final_product_list:
        subcategory_keywords = await load_external_keywords(base_config.get('external_keywords_file', ''))
        lang = base_config.get("language", "en")
        await save_products_by_subcategory(final_product_list, site_name, group_name, category_path, subcategory_keywords, lang)

    newly_added_count = len(final_product_list) - initial_count
    logging.info(f"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã {site_name.upper()} - {group_name.upper()} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. ---")
    logging.info(f"üìä –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –≥—Ä—É–ø–ø–µ: {len(final_product_list)} (–¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {newly_added_count})")
    
    return final_product_list


async def main_async():
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤.
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, 'config.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            configs = json.load(f)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config.json: {e}")
        return

    if len(sys.argv) > 1:
        target_site_name = sys.argv[1]
        configs = [c for c in configs if c['site_name'] == target_site_name]
        if not configs:
            logging.error(f"–°–∞–π—Ç '{target_site_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ config.json.")
            return
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —Å–∞–π—Ç—É –∏ –≥—Ä—É–ø–ø–µ
    grouped_configs = {}
    for config in configs:
        if config.get('enabled', True):
            group_name = config.get('group', config['category_name'])
            site_name = config['site_name']
            key = (site_name, group_name)
            if key not in grouped_configs:
                grouped_configs[key] = []
            grouped_configs[key].append(config)
        else:
            logging.warning(f"–ü–†–û–ü–£–°–ö–ê–Æ: {config['site_name'].upper()} ({config['category_name']}) (–æ—Ç–∫–ª—é—á–µ–Ω)")

    all_results = {}
    for group_key, configs_in_group in grouped_configs.items():
        group_products = await process_config_group(group_key, configs_in_group)
        all_results[f"{group_key[0]}_{group_key[1]}"] = group_products

    # ... (–æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)

def main():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    script_dir = os.path.dirname(os.path.abspath(__file__))
    log_file_path = os.path.join(script_dir, 'scraper.log')
    root_logger = logging.getLogger()
    root_logger.handlers = []
    root_logger.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s')
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setFormatter(file_formatter)
    root_logger.addHandler(file_handler)
    console_formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s', datefmt='%Y-%m-%d %H:%M')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)

    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
    asyncio.run(main_async())

    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
    logging.info(f"\n{'='*60}")
    logging.info("üßπ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ß–ò–°–¢–ö–ê –¢–û–í–ê–†–û–í")
    logging.info(f"{ '='*60}")
    try:
        from utils.clean_products import clean_file
        import glob
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
        pattern = os.path.join(output_dir, "**", "*.txt")
        files = glob.glob(pattern, recursive=True)
        if files:
            logging.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏: {len(files)}")
            success_count = sum(1 for file_path in files if clean_file(file_path))
            logging.info(f"\n‚úÖ –û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê! –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count} –∏–∑ {len(files)} —Ñ–∞–π–ª–æ–≤")
        else:
            logging.warning("‚ùå –§–∞–π–ª—ã –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–µ: {e}", exc_info=True)

if __name__ == "__main__":
    main()